{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6146,"status":"ok","timestamp":1720773780335,"user":{"displayName":"Rosencor Rosencor","userId":"03851350926450566129"},"user_tz":-120},"id":"AhbFCiyyjPjc","outputId":"832f257e-c17a-4c69-9fde-b114cf472047"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary size (words): len(y[i]): 3117\n","Total X variables: 182393\n","\n","Text train example1\n"]},{"output_type":"display_data","data":{"text/plain":["'34021kt 9999 RA FEW025 SCT 07 05 q1018 01017G29KT 340V050 7000 -SHRA FEW014 BKN020 SCT030TCU 05/03 Q1017 TEMPO 3000 SHRA BKN014'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Sequences example 1\n"]},{"output_type":"display_data","data":{"text/plain":["[1604,\n"," 1,\n"," 8,\n"," 49,\n"," 22,\n"," 16,\n"," 24,\n"," 25,\n"," 2410,\n"," 184,\n"," 100,\n"," 45,\n"," 168,\n"," 94,\n"," 147,\n"," 24,\n"," 40,\n"," 28,\n"," 10,\n"," 21,\n"," 45,\n"," 129]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Text train example2\n"]},{"output_type":"display_data","data":{"text/plain":["'01007kt 9999 WM MNClD M 14 12 q1022 VRB02KT CAVOK 14/12 Q1022 NOSIG'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Sequences example 2\n"]},{"output_type":"display_data","data":{"text/plain":["[160, 1, 2, 17, 4, 12, 6, 33, 48, 15, 12, 6, 33, 3]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","X\n"]},{"output_type":"display_data","data":{"text/plain":["array([[   0,    0,    0,    0,    0, 1604,    1,    8,   49,   22,   16,\n","          24,   25],\n","       [   0,    0,    0,    0, 1604,    1,    8,   49,   22,   16,   24,\n","          25, 2410],\n","       [   0,    0,    0, 1604,    1,    8,   49,   22,   16,   24,   25,\n","        2410,  184],\n","       [   0,    0, 1604,    1,    8,   49,   22,   16,   24,   25, 2410,\n","         184,  100],\n","       [   0, 1604,    1,    8,   49,   22,   16,   24,   25, 2410,  184,\n","         100,   45],\n","       [1604,    1,    8,   49,   22,   16,   24,   25, 2410,  184,  100,\n","          45,  168],\n","       [   1,    8,   49,   22,   16,   24,   25, 2410,  184,  100,   45,\n","         168,   94],\n","       [   8,   49,   22,   16,   24,   25, 2410,  184,  100,   45,  168,\n","          94,  147],\n","       [  49,   22,   16,   24,   25, 2410,  184,  100,   45,  168,   94,\n","         147,   24],\n","       [  22,   16,   24,   25, 2410,  184,  100,   45,  168,   94,  147,\n","          24,   40],\n","       [  16,   24,   25, 2410,  184,  100,   45,  168,   94,  147,   24,\n","          40,   28],\n","       [  24,   25, 2410,  184,  100,   45,  168,   94,  147,   24,   40,\n","          28,   10],\n","       [  25, 2410,  184,  100,   45,  168,   94,  147,   24,   40,   28,\n","          10,   21],\n","       [2410,  184,  100,   45,  168,   94,  147,   24,   40,   28,   10,\n","          21,   45],\n","       [   0,    0,    0,    0,    0,  160,    1,    2,   17,    4,   12,\n","           6,   33],\n","       [   0,    0,    0,    0,  160,    1,    2,   17,    4,   12,    6,\n","          33,   48],\n","       [   0,    0,    0,  160,    1,    2,   17,    4,   12,    6,   33,\n","          48,   15],\n","       [   0,    0,  160,    1,    2,   17,    4,   12,    6,   33,   48,\n","          15,   12],\n","       [   0,  160,    1,    2,   17,    4,   12,    6,   33,   48,   15,\n","          12,    6],\n","       [ 160,    1,    2,   17,    4,   12,    6,   33,   48,   15,   12,\n","           6,   33],\n","       [   0,    0,    0,    0,    0,  163,    1,   30,   61,    4,   32,\n","          40,   56],\n","       [   0,    0,    0,    0,  163,    1,   30,   61,    4,   32,   40,\n","          56,  351],\n","       [   0,    0,    0,  163,    1,   30,   61,    4,   32,   40,   56,\n","         351,  222],\n","       [   0,    0,  163,    1,   30,   61,    4,   32,   40,   56,  351,\n","         222,    1],\n","       [   0,  163,    1,   30,   61,    4,   32,   40,   56,  351,  222,\n","           1,  993]], dtype=int32)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Dictionary first 10 words\n"]},{"output_type":"display_data","data":{"text/plain":["{'9999': 1,\n"," 'wm': 2,\n"," 'nosig': 3,\n"," 'm': 4,\n"," '11': 5,\n"," '12': 6,\n"," '10': 7,\n"," 'ra': 8,\n"," '13': 9,\n"," 'tempo': 10}"]},"metadata":{}}],"source":["#@title Get text train and test ,X and Y\n","nrows_train = 20000 # @param {type:\"integer\"}\n","sequence_length = 13 # @param {type:\"integer\"}\n","\n","feed_lenght = 8\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.optimizers import Adam\n","import json\n","from keras.preprocessing.text import tokenizer_from_json\n","\n","#load fusion\n","fus = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/gpt/i+d/fusionml.csv\",\n","                  parse_dates=[\"time\"], index_col=\"time\")\n","\n","#Get the train and test train\n","texts_train = fus[\"fusion\"].sample(nrows_train,)\n","texts_test = fus[\"fusion\"].drop(texts_train.index)\n","\n","#save texts test\n","texts_test.to_csv(\"/content/drive/MyDrive/Colab Notebooks/gpt/i+d/texts_testml.csv\")\n","\n","# Tokenize text\n","tokenizer = Tokenizer()\n","\n","#tokenizer.fit_on_texts(texts_train)\n","tokenizer.fit_on_texts(fus[\"fusion\"])\n","\n","#Save tokenizer\n","tokenizer_json = tokenizer.to_json()\n","\n","# Save the JSON configuration to a file\n","with open('/content/drive/MyDrive/Colab Notebooks/gpt/i+d/tokenizerml.json', 'w', encoding='utf-8') as f:\n","    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n","\n","sequences = tokenizer.texts_to_sequences(texts_train)\n","\n","# Prepare input and output data\n","X = []\n","y = []\n","for sequence in sequences:\n","    for i in range(1, len(sequence)):\n","        x_seq = sequence[:i]\n","        x_seq_padded = pad_sequences([x_seq], maxlen=sequence_length, padding='pre')\n","        X.append(x_seq_padded[0])\n","        y.append(sequence[i])\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","#filter seed/model words\n","df = pd.DataFrame(X)\n","df[\"y\"] = y\n","df_fil =  df[(df.iloc[:, :sequence_length-feed_lenght+1] != 0).any(axis=1)]\n","X = df_fil.iloc[:, :-1].values\n","y = df_fil.iloc[:, -1].values\n","\n","# One hot encode the outputs\n","y = np.eye(len(tokenizer.word_index) + 1)[y]\n","\n","print(\"Dictionary size (words): len(y[i]):\",len(y[1]) )\n","print(\"Total X variables:\",len(X) )\n","\n","print(\"\\nText train example1\")\n","display(texts_train[0])\n","\n","print(\"\\nSequences example 1\")\n","display(sequences[0])\n","\n","print(\"\\nText train example2\")\n","display(texts_train[1])\n","\n","print(\"\\nSequences example 2\")\n","display(sequences[1])\n","\n","print(\"\\nX\")\n","display(X[:25])\n","\n","print(\"\\nDictionary first 10 words\")\n","display({k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]})\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fOX-9Am0V_d","executionInfo":{"status":"ok","timestamp":1720775526310,"user_tz":-120,"elapsed":1631762,"user":{"displayName":"Rosencor Rosencor","userId":"03851350926450566129"}},"outputId":"01290a04-2ef8-4886-bbf2-46a767d33e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/70\n","357/357 [==============================] - 27s 66ms/step - loss: 3.8810 - accuracy: 0.2185\n","Epoch 2/70\n","357/357 [==============================] - 23s 65ms/step - loss: 2.6541 - accuracy: 0.3320\n","Epoch 3/70\n","357/357 [==============================] - 23s 64ms/step - loss: 2.3040 - accuracy: 0.3917\n","Epoch 4/70\n","357/357 [==============================] - 23s 64ms/step - loss: 2.1503 - accuracy: 0.4186\n","Epoch 5/70\n","357/357 [==============================] - 23s 65ms/step - loss: 2.0555 - accuracy: 0.4322\n","Epoch 6/70\n","357/357 [==============================] - 23s 66ms/step - loss: 1.9797 - accuracy: 0.4452\n","Epoch 7/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.9171 - accuracy: 0.4554\n","Epoch 8/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.8535 - accuracy: 0.4665\n","Epoch 9/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.8000 - accuracy: 0.4751\n","Epoch 10/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.7427 - accuracy: 0.4882\n","Epoch 11/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.6940 - accuracy: 0.4976\n","Epoch 12/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.6446 - accuracy: 0.5093\n","Epoch 13/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.5971 - accuracy: 0.5193\n","Epoch 14/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.5588 - accuracy: 0.5298\n","Epoch 15/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.5174 - accuracy: 0.5396\n","Epoch 16/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.4823 - accuracy: 0.5471\n","Epoch 17/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.4493 - accuracy: 0.5551\n","Epoch 18/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.4229 - accuracy: 0.5627\n","Epoch 19/70\n","357/357 [==============================] - 23s 66ms/step - loss: 1.3896 - accuracy: 0.5703\n","Epoch 20/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.3633 - accuracy: 0.5778\n","Epoch 21/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.3447 - accuracy: 0.5818\n","Epoch 22/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.3261 - accuracy: 0.5874\n","Epoch 23/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.3003 - accuracy: 0.5930\n","Epoch 24/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.2960 - accuracy: 0.5935\n","Epoch 25/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.2644 - accuracy: 0.6017\n","Epoch 26/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.2461 - accuracy: 0.6071\n","Epoch 27/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.2300 - accuracy: 0.6120\n","Epoch 28/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.2190 - accuracy: 0.6136\n","Epoch 29/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.2058 - accuracy: 0.6177\n","Epoch 30/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1913 - accuracy: 0.6205\n","Epoch 31/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1795 - accuracy: 0.6251\n","Epoch 32/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1742 - accuracy: 0.6259\n","Epoch 33/70\n","357/357 [==============================] - 23s 66ms/step - loss: 1.1644 - accuracy: 0.6293\n","Epoch 34/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1742 - accuracy: 0.6241\n","Epoch 35/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1573 - accuracy: 0.6292\n","Epoch 36/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1491 - accuracy: 0.6303\n","Epoch 37/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1449 - accuracy: 0.6312\n","Epoch 38/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1313 - accuracy: 0.6348\n","Epoch 39/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1282 - accuracy: 0.6361\n","Epoch 40/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1311 - accuracy: 0.6356\n","Epoch 41/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1163 - accuracy: 0.6401\n","Epoch 42/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1139 - accuracy: 0.6392\n","Epoch 43/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1191 - accuracy: 0.6375\n","Epoch 44/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1140 - accuracy: 0.6390\n","Epoch 45/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1011 - accuracy: 0.6421\n","Epoch 46/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1077 - accuracy: 0.6401\n","Epoch 47/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1035 - accuracy: 0.6414\n","Epoch 48/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1113 - accuracy: 0.6398\n","Epoch 49/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1085 - accuracy: 0.6381\n","Epoch 50/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0947 - accuracy: 0.6434\n","Epoch 51/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1014 - accuracy: 0.6422\n","Epoch 52/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1183 - accuracy: 0.6375\n","Epoch 53/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1164 - accuracy: 0.6365\n","Epoch 54/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1042 - accuracy: 0.6402\n","Epoch 55/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0993 - accuracy: 0.6408\n","Epoch 56/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0961 - accuracy: 0.6415\n","Epoch 57/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0819 - accuracy: 0.6462\n","Epoch 58/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0966 - accuracy: 0.6408\n","Epoch 59/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1388 - accuracy: 0.6303\n","Epoch 60/70\n","357/357 [==============================] - 23s 66ms/step - loss: 1.1375 - accuracy: 0.6304\n","Epoch 61/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1040 - accuracy: 0.6389\n","Epoch 62/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.1078 - accuracy: 0.6361\n","Epoch 63/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.0980 - accuracy: 0.6391\n","Epoch 64/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0884 - accuracy: 0.6435\n","Epoch 65/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0959 - accuracy: 0.6403\n","Epoch 66/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1000 - accuracy: 0.6393\n","Epoch 67/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.1141 - accuracy: 0.6362\n","Epoch 68/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0931 - accuracy: 0.6408\n","Epoch 69/70\n","357/357 [==============================] - 23s 64ms/step - loss: 1.0955 - accuracy: 0.6398\n","Epoch 70/70\n","357/357 [==============================] - 23s 65ms/step - loss: 1.0982 - accuracy: 0.6391\n"]}],"source":["#@title Train the model\n","\n","from keras.layers import Bidirectional\n","\n","# Build the LSTM model\n","model = Sequential([\n","    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=20, input_length=sequence_length),\n","    Bidirectional(LSTM(130)),\n","\n","    Dense(len(tokenizer.word_index)+1, activation='softmax')\n","])\n","\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, epochs=70,batch_size=512)\n","\n","#save the model\n","model.save(\"/content/drive/MyDrive/Colab Notebooks/gpt/i+d/model_ml.keras\")\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[{"file_id":"1PKiCntrB2pBLVWZRaL6x-uJpEvPlNmAg","timestamp":1717661221746},{"file_id":"1qsxpGdTrTbO4obEJ4QwGeDJUpuX7gXoW","timestamp":1717492644174},{"file_id":"1LGw-aME2JOnpzAyMOVamYWUAywgSV5mI","timestamp":1715852482288},{"file_id":"1_9maObId68xlnKd6JjYqJUMRMI14ASnx","timestamp":1715845357828},{"file_id":"1BterrhINI5z4G_D4Ntuk7p8a0iuZu7ql","timestamp":1714808806688},{"file_id":"1sqdm_O_jJnvjiOLxkCsuGODjv7CDiB0l","timestamp":1714549345450},{"file_id":"1GD83k4KMSFWH42Th2LjdPwK-hN39h8RT","timestamp":1713425984380}],"mount_file_id":"105ze3Z0kuqhnndeEZkjJaWZXCN-Zk0ym","authorship_tag":"ABX9TyPyhYsT80nBMTbIg4uIOF9B"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}